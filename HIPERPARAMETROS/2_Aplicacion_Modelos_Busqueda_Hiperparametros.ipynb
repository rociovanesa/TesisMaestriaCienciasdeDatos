{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735f7e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ahora modelado y analsis comparativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93d71a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_12952\\3178245203.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtrado['SubRubro_encoded_relabel'] = le2.fit_transform(data_filtrado['SubRubro_encoded'])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "#Luego, cuando filtramos las clases con ‚â•2 ejemplos, algunas etiquetas desaparecieron \n",
    "#XGBoost no tolera esos saltos.Hay que relabel las clases despu√©s de filtrar para que sean continuas (0, 1, 2, 3, 4, 5,...).\n",
    "#volver a usar LabelEncoder despu√©s de filtrar.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Variables\n",
    "X = tfidf_dataframe\n",
    "y = data['SubRubro_encoded']\n",
    "\n",
    "#***********agrego esto porque daba error\n",
    "\n",
    "# Contar cu√°ntas veces aparece cada clase\n",
    "conteo_clases = data['SubRubro_encoded'].value_counts()\n",
    "\n",
    "# Filtrar para quedarnos solo con las clases que tienen al menos 2 productos\n",
    "clases_validas = conteo_clases[conteo_clases >= 2].index\n",
    "\n",
    "# Filtrar el dataset\n",
    "data_filtrado = data[data['SubRubro_encoded'].isin(clases_validas)]\n",
    "\n",
    "# *** NUEVO: Reasignar etiquetas consecutivas ***\n",
    "le2 = LabelEncoder()\n",
    "data_filtrado['SubRubro_encoded_relabel'] = le2.fit_transform(data_filtrado['SubRubro_encoded'])\n",
    "# SubRubro_encoded_relabel es una nueva columna con las etiquetas consecutivas correctas.\n",
    "\n",
    "# Redefinir X e y\n",
    "X = tfidf_vectorizer.transform(data_filtrado['Nombre_Limpio']).toarray()\n",
    "y = data_filtrado['SubRubro_encoded_relabel']  # Ahora usamos la versi√≥n \"relabel\"\n",
    "#**********************************************\n",
    "\n",
    "\n",
    "# Definir todas las clases posibles (importante para XGBoost)\n",
    "todas_las_clases = np.unique(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4a4c25a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_1408\\2656807289.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtrado['SubRubro_encoded_relabel'] = le2.fit_transform(data_filtrado['SubRubro_encoded'])\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "#     Aplicaci√≥n de modelos\n",
    "# -------------------------------------\n",
    "# ‚ö†Ô∏èACLARACIONES:\n",
    "# Aqui me dio error porque algunas clases Tienes clases en SubRubro_encoded que solo tienen 1 ejemplo.\n",
    "# Cuando pides stratify=y, Sklearn necesita m√≠nimo 2 muestras por clase para poder dividir entre train y test.\n",
    "# No se puede dividir 1 solo dato en train/test, por eso explota.\n",
    "# el dataset tiene algunos subrubros muy raros o casi vac√≠os (con 1 solo producto).\n",
    "# Esto complica el modelado porque no se puede entrenar ni testear bien con una clase as√≠.\n",
    "# entonces debi eliminar el dataset aquellas clases (SubRubro_encoded) que tienen menos de 2 ejemplos\n",
    "# -------------------------------------\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "#Luego, cuando filtramos las clases con ‚â•2 ejemplos, algunas etiquetas desaparecieron \n",
    "#XGBoost no tolera esos saltos.Hay que relabel las clases despu√©s de filtrar para que sean continuas (0, 1, 2, 3, 4, 5,...).\n",
    "#volver a usar LabelEncoder despu√©s de filtrar.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Variables\n",
    "X = tfidf_dataframe\n",
    "y = data['SubRubro_encoded']\n",
    "\n",
    "#***********agrego esto porque daba error\n",
    "\n",
    "# Contar cu√°ntas veces aparece cada clase\n",
    "conteo_clases = data['SubRubro_encoded'].value_counts()\n",
    "\n",
    "# Filtrar para quedarnos solo con las clases que tienen al menos 2 productos\n",
    "clases_validas = conteo_clases[conteo_clases >= 2].index\n",
    "\n",
    "# Filtrar el dataset\n",
    "data_filtrado = data[data['SubRubro_encoded'].isin(clases_validas)]\n",
    "\n",
    "# *** NUEVO: Reasignar etiquetas consecutivas ***\n",
    "le2 = LabelEncoder()\n",
    "data_filtrado['SubRubro_encoded_relabel'] = le2.fit_transform(data_filtrado['SubRubro_encoded'])\n",
    "# SubRubro_encoded_relabel es una nueva columna con las etiquetas consecutivas correctas.\n",
    "\n",
    "# Redefinir X e y\n",
    "X = tfidf_vectorizer.transform(data_filtrado['Nombre_Limpio']).toarray()\n",
    "y = data_filtrado['SubRubro_encoded_relabel']  # Ahora usamos la versi√≥n \"relabel\"\n",
    "#**********************************************\n",
    "\n",
    "\n",
    "\n",
    "# Definir todas las clases posibles (importante para XGBoost)\n",
    "todas_las_clases = np.unique(y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5c3624",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NUEVOS MODELOS##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49faea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "#   üîéüîé ‚úîÔ∏èVALIDACI√ìN\n",
    "#  Busca de los hipermarametros para Random Forest\n",
    "#--------------------------------------------------\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "\n",
    "# Datos\n",
    "X = tfidf_vectorizer.transform(data_filtrado['Nombre_Limpio']).toarray()\n",
    "y = data_filtrado['SubRubro_encoded_relabel']\n",
    "\n",
    "# Definir la cuadr√≠cula de hiperpar√°metros\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Instanciar modelo base\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Instanciar GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf,\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5,  # validaci√≥n cruzada de 5 folds\n",
    "                           scoring='f1_weighted',\n",
    "                           n_jobs=-1,  # paraleliza el proceso\n",
    "                           verbose=2)\n",
    "\n",
    "# Ejecutar b√∫squeda\n",
    "inicio = time.time()\n",
    "grid_search.fit(X, y)\n",
    "fin = time.time()\n",
    "\n",
    "# Resultados\n",
    "print(\"\\nMejores hiperpar√°metros:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "print(f\"\\nMejor F1-Score (cv): {grid_search.best_score_:.4f}\")\n",
    "print(f\"Tiempo total de b√∫squeda: {fin - inicio:.2f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0dc02d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados por pliegue:\n",
      "   Accuracy  Precisi√≥n  Recall  F1-Score  Tiempo (s)\n",
      "0    0.9945     0.5512  0.5059    0.5459     14.5207\n",
      "1    0.9951     0.5794  0.5613    0.6171     14.7311\n",
      "2    0.9945     0.5300  0.5103    0.5719     15.1886\n",
      "3    0.9944     0.5451  0.5026    0.5625     15.0333\n",
      "4    0.9953     0.6103  0.5970    0.6454     13.9368\n",
      "\n",
      "Promedios:\n",
      "   Accuracy  Precisi√≥n  Recall  F1-Score  Tiempo (s)\n",
      "0    0.9948     0.5632  0.5354    0.5885     14.6821\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------\n",
    "#   üíªüí• ‚úîÔ∏èVALIDACI√ìN\n",
    "#  RANDOM FOREST - CROSS VALIDACION CON hiperparametros optimos\n",
    "#-------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "# --- Datos ---\n",
    "X = tfidf_vectorizer.transform(data_filtrado['Nombre_Limpio']).toarray()\n",
    "y = data_filtrado['SubRubro_encoded_relabel'].reset_index(drop=True)\n",
    "\n",
    "# --- Modelo con mejores hiperpar√°metros ---\n",
    "rf_model = RandomForestClassifier(\n",
    "    bootstrap=True,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=5,\n",
    "    n_estimators=100,#era 100\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- Validaci√≥n cruzada ---\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "resultados = []\n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    inicio = time.time()\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    fin = time.time()\n",
    "\n",
    "    # Matriz de confusi√≥n global (todas clases)\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "    TP = np.diag(cm)\n",
    "    FP = cm.sum(axis=0) - TP\n",
    "    FN = cm.sum(axis=1) - TP\n",
    "    TN = cm.sum() - (TP + FP + FN)\n",
    "\n",
    "    # Evitar divisiones por cero\n",
    "    precision_clase = np.divide(TP, TP + FP, out=np.zeros_like(TP, dtype=float), where=(TP + FP) != 0)\n",
    "    recall_clase = np.divide(TP, TP + FN, out=np.zeros_like(TP, dtype=float), where=(TP + FN) != 0)\n",
    "    accuracy_clase = np.divide(TP + TN, TP + TN + FP + FN, out=np.zeros_like(TP, dtype=float), where=(TP + TN + FP + FN) != 0)\n",
    "\n",
    "    # Promedios macro por pliegue\n",
    "    acc = accuracy_clase.mean()\n",
    "    prec = precision_clase.mean()\n",
    "    rec = recall_clase.mean()\n",
    "    f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    tiempo = fin - inicio\n",
    "\n",
    "    resultados.append({\n",
    "        'Accuracy': acc,\n",
    "        'Precisi√≥n': prec,\n",
    "        'Recall': rec,\n",
    "        'F1-Score': f1,\n",
    "        'Tiempo (s)': tiempo\n",
    "    })\n",
    "\n",
    "# --- Mostrar resultados por pliegue ---\n",
    "resultados_df = pd.DataFrame(resultados)\n",
    "print(\"\\nResultados por pliegue:\")\n",
    "print(resultados_df.round(4))\n",
    "\n",
    "# --- Promedios ---\n",
    "promedios = resultados_df.mean().to_frame().T\n",
    "print(\"\\nPromedios:\")\n",
    "print(promedios.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7457cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------\n",
    "#   üîé VALIDACI√ìN\n",
    "#  Busca de los hipermarametros para SVM\n",
    "#----------------------------------------\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import time\n",
    "\n",
    "# 1. Datos\n",
    "X = tfidf_vectorizer.transform(data_filtrado['Nombre_Limpio']).toarray()\n",
    "y = data_filtrado['SubRubro_encoded_relabel']\n",
    "\n",
    "# 2. Definir la grilla de hiperpar√°metros\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],                     # Regularizaci√≥n\n",
    "    'kernel': ['linear', 'rbf'],          # Tipos de kernel\n",
    "    'gamma': ['scale', 'auto']            # Solo para kernels no lineales\n",
    "}\n",
    "\n",
    "# 3. Definir modelo base\n",
    "modelo_svm = SVC()\n",
    "\n",
    "# 4. Definir GridSearchCV con F1-score como m√©trica\n",
    "grid = GridSearchCV(estimator=modelo_svm,\n",
    "                    param_grid=param_grid,\n",
    "                    scoring=make_scorer(f1_score, average='weighted'),\n",
    "                    cv=5,\n",
    "                    n_jobs=-1,\n",
    "                    verbose=2)\n",
    "\n",
    "# 5. Ajuste con control de tiempo\n",
    "inicio = time.time()\n",
    "grid.fit(X, y)\n",
    "fin = time.time()\n",
    "\n",
    "# 6. Resultados\n",
    "print(\"\\nMejores hiperpar√°metros:\")\n",
    "print(grid.best_params_)\n",
    "print(f\"\\nMejor F1-Score (cv): {grid.best_score_:.4f}\")\n",
    "print(f\"Tiempo total de b√∫squeda: {fin - inicio:.2f} segundos\")\n",
    "\n",
    "#Mejores hiperpar√°metros:\n",
    "#{'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n",
    "\n",
    "#Mejor F1-Score (cv): 0.6211\n",
    "#Tiempo total de b√∫squeda: 2122.02 segundos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6712907b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados por pliegue:\n",
      "   Pliegue  Accuracy  Precisi√≥n  Recall  F1-Score  Tiempo (s)\n",
      "0        1    0.6885     0.7086  0.5055    0.6764     29.0638\n",
      "1        2    0.7154     0.7287  0.5941    0.7040     28.8397\n",
      "2        3    0.6996     0.7311  0.5443    0.6936     29.0091\n",
      "3        4    0.6829     0.7110  0.5278    0.6727     29.1496\n",
      "4        5    0.7497     0.7603  0.6466    0.7387     30.1970\n",
      "\n",
      "Promedios:\n",
      "Accuracy       0.7072\n",
      "Precisi√≥n      0.7279\n",
      "Recall         0.5636\n",
      "F1-Score       0.6971\n",
      "Tiempo (s)    29.2518\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------\n",
    "#    üíªüí• ‚úîÔ∏èVALIDACI√ìN VALIDACI√ìN\n",
    "#  Aplicaci√≥n Cross Validation con mejores hiperpar√°metros para SVM\n",
    "#------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sup√≥n que tienes el DataFrame `data_filtrado` ya cargado\n",
    "# Paso 1: TF-IDF y extracci√≥n de variables\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words='english')\n",
    "X = tfidf_vectorizer.fit_transform(data_filtrado['Nombre_Limpio']).toarray()\n",
    "y = data_filtrado['SubRubro_encoded_relabel'].reset_index(drop=True)\n",
    "\n",
    "# Paso 2: Definir modelo SVM con los mejores hiperpar√°metros\n",
    "modelo = SVC(C=10, gamma='scale', kernel='linear')\n",
    "\n",
    "# Paso 3: Validaci√≥n cruzada\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "resultados = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X, y), start=1):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    inicio = time.time()\n",
    "    modelo.fit(X_train, y_train)\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    fin = time.time()\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    tiempo = fin - inicio\n",
    "\n",
    "    resultados.append({\n",
    "        'Pliegue': i,\n",
    "        'Accuracy': acc,\n",
    "        'Precisi√≥n': prec,\n",
    "        'Recall': rec,\n",
    "        'F1-Score': f1,\n",
    "        'Tiempo (s)': tiempo\n",
    "    })\n",
    "\n",
    "# Mostrar resultados\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "print(\"\\nResultados por pliegue:\")\n",
    "print(df_resultados.round(4))\n",
    "\n",
    "print(\"\\nPromedios:\")\n",
    "print(df_resultados[['Accuracy', 'Precisi√≥n', 'Recall', 'F1-Score', 'Tiempo (s)']].mean().round(4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169e4512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------\n",
    "#   üîéüîé ‚úîÔ∏è VALIDACI√ìN\n",
    "#  Busca de los hipermarametros para Regresi√≥n Log√≠stica\n",
    "#----------------------------------------\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import time\n",
    "\n",
    "# --- Definir el espacio de b√∫squeda de hiperpar√°metros ---\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],            # Fuerza de regularizaci√≥n\n",
    "    'penalty': ['l2'],                       # Penalizaci√≥n (l1 requiere 'liblinear')\n",
    "    'solver': ['lbfgs', 'saga'],             # Algoritmos que soportan 'l2' y multiclase\n",
    "    'multi_class': ['multinomial'],          # Para clasificaci√≥n multiclase\n",
    "    'max_iter': [500, 1000]                  # Iteraciones m√°ximas para convergencia\n",
    "}\n",
    "\n",
    "# --- Instanciar modelo base ---\n",
    "modelo_base = LogisticRegression()\n",
    "\n",
    "# --- Definir el GridSearch ---\n",
    "grid = GridSearchCV(\n",
    "    estimator=modelo_base,\n",
    "    param_grid=param_grid,\n",
    "    scoring=make_scorer(f1_score, average='weighted'),\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# --- Ejecutar b√∫squeda ---\n",
    "inicio = time.time()\n",
    "grid.fit(X_train, y_train)\n",
    "fin = time.time()\n",
    "\n",
    "# --- Mostrar resultados ---\n",
    "print(\"Mejores hiperpar√°metros:\")\n",
    "print(grid.best_params_)\n",
    "print(f\"\\nMejor F1-Score (cv): {round(grid.best_score_, 4)}\")\n",
    "print(f\"Tiempo total de b√∫squeda: {round(fin - inicio, 2)} segundos\")\n",
    "\n",
    "# Mejores hiperpar√°metros:\n",
    "# {'C': 100, 'max_iter': 1000, 'multi_class': 'multinomial', 'penalty': 'l2', 'solver': 'saga'}\n",
    "\n",
    "# Mejor F1-Score (cv): 0.6274\n",
    "# Tiempo total de b√∫squeda: 53892.58 segundos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d236c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #-------------------------------------------------------------------------\n",
    "#   üí•üíª‚ö†Ô∏è VALIDACI√ìN\n",
    "#  REGRESI√ìN LOG√çSTICA - Cross validacion con  los hipermarametros √≥ptimos\n",
    "# #-------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Datos\n",
    "X = tfidf_vectorizer.transform(data_filtrado['Nombre_Limpio']).toarray()\n",
    "y = data_filtrado['SubRubro_encoded_relabel'].reset_index(drop=True)\n",
    "\n",
    "# Modelo con mejores hiperpar√°metros\n",
    "modelo = LogisticRegression(\n",
    "    C=100,\n",
    "    max_iter=1000,\n",
    "    multi_class='multinomial',\n",
    "    penalty='l2',\n",
    "    solver='saga'\n",
    ")\n",
    "\n",
    "# Validaci√≥n cruzada\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "resultados = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f\"Pliegue {fold+1}\")\n",
    "    \n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    inicio = time.time()\n",
    "    modelo.fit(X_train, y_train)\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    fin = time.time()\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    tiempo = fin - inicio\n",
    "    \n",
    "    resultados.append({\n",
    "        'Accuracy': acc,\n",
    "        'Precisi√≥n': prec,\n",
    "        'Recall': rec,\n",
    "        'F1-Score': f1,\n",
    "        'Tiempo (s)': tiempo\n",
    "    })\n",
    "\n",
    "# Mostrar resultados\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "print(\"\\nResultados por pliegue:\")\n",
    "print(df_resultados.round(4))\n",
    "\n",
    "print(\"\\nPromedios:\")\n",
    "print(df_resultados.mean().round(4))\n",
    "\n",
    "# Pliegue 5\n",
    "\n",
    "# Resultados por pliegue:\n",
    "#    Accuracy  Precisi√≥n  Recall  F1-Score  Tiempo (s)\n",
    "# 0    0.7064     0.5793  0.5241    0.5339   6896.2770\n",
    "# 1    0.7423     0.6337  0.5985    0.5907   6948.1079\n",
    "# 2    0.7137     0.5951  0.5395    0.5486   6898.3654\n",
    "# 3    0.7060     0.5888  0.5347    0.5399   6870.5371\n",
    "# 4    0.7497     0.6183  0.6093    0.5970   6869.4316\n",
    "\n",
    "# Promedios:\n",
    "# Accuracy         0.7236\n",
    "# Precisi√≥n        0.6031\n",
    "# Recall           0.5612\n",
    "# F1-Score         0.5620\n",
    "# Tiempo (s)    6896.5438\n",
    "# dtype: float64\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51878dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------\n",
    "#   üîéüîé ‚úîÔ∏è VALIDACI√ìN\n",
    "#  Busca de los hipermarametros para Naive Bayes\n",
    "#----------------------------------------\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import time\n",
    "\n",
    "# --- Datos (usamos los mismos ya preparados) ---\n",
    "X = tfidf_vectorizer.transform(data_filtrado['Nombre_Limpio']).toarray()\n",
    "y = data_filtrado['SubRubro_encoded_relabel']\n",
    "\n",
    "# --- Modelo base ---\n",
    "modelo = MultinomialNB()\n",
    "\n",
    "# --- Definir la grilla de par√°metros ---\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.3, 0.5, 0.7, 1.0]\n",
    "}\n",
    "\n",
    "# --- Configurar GridSearchCV ---\n",
    "grid_search_nb = GridSearchCV(\n",
    "    estimator=modelo,\n",
    "    param_grid=param_grid,\n",
    "    scoring=make_scorer(f1_score, average='weighted'),\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# --- Ejecutar b√∫squeda ---\n",
    "inicio = time.time()\n",
    "grid_search_nb.fit(X, y)\n",
    "fin = time.time()\n",
    "\n",
    "# --- Resultados ---\n",
    "print(\"Mejores hiperpar√°metros:\")\n",
    "print(grid_search_nb.best_params_)\n",
    "\n",
    "print(f\"\\nMejor F1-Score (cv): {grid_search_nb.best_score_:.4f}\")\n",
    "print(f\"Tiempo total de b√∫squeda: {fin - inicio:.2f} segundos\")\n",
    "\n",
    "#Mejores hiperpar√°metros:\n",
    "#{'alpha': 0.1}\n",
    "\n",
    "#Mejor F1-Score (cv): 0.6143\n",
    "#Tiempo total de b√∫squeda: 11.79 segundos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34616156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   üí•üíª‚ö†Ô∏è VALIDACI√ìN --------------------------------------------\n",
    "#  NAIVE BAYES - Aplicaci√≥n de los hiperparametros encontrados  \n",
    "#   Cross Validation\n",
    "#------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# --- Paso 1: Vectorizaci√≥n TF-IDF ---\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "X = tfidf_vectorizer.fit_transform(data_filtrado['Nombre_Limpio']).toarray()\n",
    "y = data_filtrado['SubRubro_encoded_relabel'].reset_index(drop=True)\n",
    "\n",
    "# --- Paso 2: Modelo con mejor hiperpar√°metro ---\n",
    "modelo = MultinomialNB(alpha=0.1)\n",
    "\n",
    "# --- Paso 3: Validaci√≥n cruzada ---\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "resultados = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X, y), start=1):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    inicio = time.time()\n",
    "    modelo.fit(X_train, y_train)\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    fin = time.time()\n",
    "\n",
    "    # Matriz de confusi√≥n\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "    support = cm.sum(axis=1)\n",
    "\n",
    "    TP = np.diag(cm)\n",
    "    FP = cm.sum(axis=0) - TP\n",
    "    FN = cm.sum(axis=1) - TP\n",
    "    TN = cm.sum() - (TP + FP + FN)\n",
    "\n",
    "    # Accuracy global\n",
    "    \n",
    "    acc = np.trace(cm) / np.sum(cm) \n",
    "\n",
    "    # M√©tricas por clase\n",
    "    precision_clase = np.divide(TP, TP + FP, out=np.zeros_like(TP, dtype=float), where=(TP + FP) != 0)\n",
    "    recall_clase = np.divide(TP, TP + FN, out=np.zeros_like(TP, dtype=float), where=(TP + FN) != 0)\n",
    "    f1_clase = np.divide(2 * precision_clase * recall_clase, precision_clase + recall_clase,\n",
    "                         out=np.zeros_like(precision_clase, dtype=float), where=(precision_clase + recall_clase) != 0)\n",
    "\n",
    "    # Promedios ponderados\n",
    "    total = support.sum()\n",
    "    prec = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    # prec = np.sum(precision_clase * support) / total\n",
    "    # rec = np.sum(recall_clase * support) / total\n",
    "    f1 = np.sum(f1_clase * support) / total\n",
    "    tiempo = fin - inicio\n",
    "\n",
    "    resultados.append({\n",
    "        'Accuracy': acc,\n",
    "        'Precisi√≥n': prec,\n",
    "        'Recall': rec,\n",
    "        'F1-Score': f1,\n",
    "        'Tiempo (s)': tiempo\n",
    "    })\n",
    "\n",
    "    \n",
    "\n",
    "# --- Mostrar resultados ---\n",
    "resultados_df = pd.DataFrame(resultados)\n",
    "print(\"\\nResultados por pliegue:\")\n",
    "print(resultados_df.round(4))\n",
    "\n",
    "print(\"\\nPromedios:\")\n",
    "print(resultados_df.mean().to_frame().T.round(4))\n",
    "\n",
    "# Resultados por pliegue:\n",
    "#    Accuracy  Precisi√≥n  Recall  F1-Score  Tiempo (s)\n",
    "# 0    0.6782     0.5222  0.4289    0.6458      0.1042\n",
    "# 1    0.7090     0.5199  0.4366    0.6768      0.0794\n",
    "# 2    0.6881     0.5149  0.4095    0.6619      0.0939\n",
    "# 3    0.6765     0.5131  0.4291    0.6449      0.0945\n",
    "# 4    0.7227     0.5451  0.4689    0.6948      0.1098\n",
    "\n",
    "# Promedios:\n",
    "#    Accuracy  Precisi√≥n  Recall  F1-Score  Tiempo (s)\n",
    "# 0    0.6949      0.523  0.4346    0.6648      0.0964\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d8a38a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [12:19:59] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Mejores hiperpar√°metros encontrados:\n",
      "{'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.2, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "üìà Mejor F1-Score (cv): 0.4491\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------\n",
    "#   üîé VALIDACI√ìN\n",
    "#  Busca de los hipermarametros para XGBoost - \n",
    "#---------------------------------------------\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "#from sklearn.utils.fixes import loguniform\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# --- Datos: TF-IDF y etiquetas ---\n",
    "X = tfidf_vectorizer.transform(data_filtrado['Nombre_Limpio']).toarray()\n",
    "y = data_filtrado['SubRubro_encoded_relabel']\n",
    "\n",
    "# --- Modelo base ---\n",
    "xgb = XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    objective='multi:softmax',\n",
    "    num_class=len(np.unique(y)),\n",
    "    verbosity=1\n",
    ")\n",
    "\n",
    "# --- Grid m√°s reducido (36 combinaciones en lugar de 384 o m√°s) ---\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150],\n",
    "    'max_depth': [4, 6],\n",
    "    'learning_rate': [0.1, 0.2],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8],\n",
    "    'gamma': [0],\n",
    "    'reg_lambda': [1],\n",
    "    'min_child_weight': [1]\n",
    "}\n",
    "\n",
    "# --- GridSearchCV con logs visibles ---\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_weighted',\n",
    "    cv=3,\n",
    "    verbose=2,  # Muestra progreso\n",
    "    n_jobs=-1   # Usa todos los n√∫cleos disponibles\n",
    ")\n",
    "\n",
    "# --- Ajustar modelo ---\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# --- Resultados ---\n",
    "print(\"\\n‚úÖ Mejores hiperpar√°metros encontrados:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"üìà Mejor F1-Score (cv): {grid_search.best_score_:.4f}\")\n",
    "\n",
    "#Mejores hiperpar√°metros encontrados:\n",
    "#{'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.2, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 150, 'reg_lambda': 1, 'subsample': 0.8}\n",
    "#üìà Mejor F1-Score (cv): 0.4491\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b2ddb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pliegue 1\n",
      "Pliegue 2\n",
      "Pliegue 3\n",
      "Pliegue 4\n",
      "Pliegue 5\n",
      "\n",
      "Resultados por pliegue:\n",
      "   Pliegue  Accuracy  Precisi√≥n  Recall  F1-Score  Tiempo (s)\n",
      "0        1    0.5474     0.2831  0.2583    0.2586     56.3128\n",
      "1        2    0.5808     0.3228  0.3002    0.2917     72.6567\n",
      "2        3    0.5481     0.2983  0.2680    0.2721     75.6915\n",
      "3        4    0.5456     0.3117  0.2836    0.2875     85.5765\n",
      "4        5    0.5751     0.3313  0.3073    0.2981     89.7105\n",
      "\n",
      "Promedios:\n",
      "   Accuracy  Precisi√≥n  Recall  F1-Score  Tiempo (s)\n",
      "0    0.5594     0.3094  0.2835    0.2816     75.9896\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------\n",
    "#      üí•üíª‚ö†Ô∏è VALIDACI√ìN\n",
    "#  XGBoost Aplicaci√≥n de los hiperparametros encontrados \n",
    "# Con Cros Validation\n",
    "#----------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Datos\n",
    "X = tfidf_vectorizer.transform(data_filtrado['Nombre_Limpio']).toarray()\n",
    "y = data_filtrado['SubRubro_encoded_relabel'].reset_index(drop=True)\n",
    "\n",
    "# Modelo con mejores hiperpar√°metros\n",
    "modelo_xgb = XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    objective='multi:softmax',\n",
    "    num_class=len(np.unique(y)),\n",
    "    colsample_bytree=0.8,\n",
    "    gamma=0,\n",
    "    learning_rate=0.2,\n",
    "    max_depth=6,\n",
    "    min_child_weight=1,\n",
    "    n_estimators=150, \n",
    "    reg_lambda=1,\n",
    "    subsample=0.8,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "# Validaci√≥n cruzada\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "resultados = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f\"Pliegue {fold+1}\")\n",
    "    \n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    inicio = time.time()\n",
    "    modelo.fit(X_train, y_train)\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    fin = time.time()\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    tiempo = fin - inicio\n",
    "    \n",
    "    resultados.append({\n",
    "         'Pliegue': fold + 1,\n",
    "        'Accuracy': acc,\n",
    "        'Precisi√≥n': prec,\n",
    "        'Recall': rec,\n",
    "        'F1-Score': f1,\n",
    "        'Tiempo (s)': tiempo\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Resultados\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "print(\"\\nResultados por pliegue:\")\n",
    "print(df_resultados.round(4))\n",
    "\n",
    "print(\"\\nPromedios:\")\n",
    "\n",
    "\n",
    "\n",
    "print(df_resultados.drop(columns='Pliegue').mean().to_frame().T.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e600dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------\n",
    "# ‚è±Ô∏èüîéüîé‚úîÔ∏è Busqueda de hiperparametros √≥ptimos para knn\n",
    "#-----------------------------------------\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import time\n",
    "\n",
    "# Definir el modelo base\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Definir la grilla de hiperpar√°metros a probar\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "# Usar F1-Score como m√©trica de evaluaci√≥n\n",
    "scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "# Configurar la b√∫squeda con validaci√≥n cruzada\n",
    "grid_search = GridSearchCV(estimator=knn,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring=scorer,\n",
    "                           cv=5,\n",
    "                           verbose=2,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "# Tiempo de ejecuci√≥n\n",
    "inicio = time.time()\n",
    "grid_search.fit(X, y)\n",
    "fin = time.time()\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Mejores hiperpar√°metros encontrados:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"\\nüìà Mejor F1-Score (cv): {grid_search.best_score_:.4f}\")\n",
    "print(f\"‚è±Ô∏è Tiempo total de b√∫squeda: {fin - inicio:.2f} segundos\")\n",
    "\n",
    "#Mejores hiperpar√°metros encontrados:\n",
    "#{'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n",
    "\n",
    "#üìà Mejor F1-Score (cv): 0.5675\n",
    "#‚è±Ô∏è Tiempo total de b√∫squeda: 263.99 segundos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf681b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "#    üí•üíª VALIDACI√ìN\n",
    "#  KNN Aplicaci√≥n de los hiperparametros encontrados \n",
    "# Cross Validation\n",
    "#----------------------------------------\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Mejor configuraci√≥n hallada\n",
    "mejores_params = {\n",
    "    'n_neighbors': 3,\n",
    "    'weights': 'distance',\n",
    "    'metric': 'euclidean'\n",
    "}\n",
    "\n",
    "# Inicializar modelo con mejores par√°metros\n",
    "modelo_knn = KNeighborsClassifier(**mejores_params)\n",
    "\n",
    "# Inicializar validaci√≥n cruzada\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Almacenar resultados por pliegue\n",
    "resultados_knn = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X, y):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    inicio = time.time()\n",
    "    modelo_knn.fit(X_train, y_train)\n",
    "    y_pred = modelo_knn.predict(X_test)\n",
    "    fin = time.time()\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    tiempo = fin - inicio\n",
    "\n",
    "    resultados_knn.append({\n",
    "        'Accuracy': acc,\n",
    "        'Precisi√≥n': prec,\n",
    "        'Recall': rec,\n",
    "        'F1-Score': f1,\n",
    "        'Tiempo (s)': tiempo\n",
    "    })\n",
    "\n",
    "# Mostrar resultados\n",
    "df_resultados_knn = pd.DataFrame(resultados_knn)\n",
    "print(df_resultados_knn.round(4))\n",
    "promedios = df_resultados_knn.mean().to_frame().T\n",
    "promedios.index = ['Promedio']\n",
    "print(promedios.round(4))\n",
    "\n",
    "#Accuracy  Precisi√≥n  Recall  F1-Score  Tiempo (s)\n",
    "#0    0.5808     0.7656  0.5808    0.6469      0.2812\n",
    "#1    0.6026     0.7742  0.6026    0.6631      0.2846\n",
    "#2    0.5892     0.7701  0.5892    0.6533      0.3040\n",
    "#3    0.6021     0.7786  0.6021    0.6581      0.2986\n",
    "#4    0.6367     0.8099  0.6367    0.6980      0.2644\n",
    " #         Accuracy  Precisi√≥n  Recall  F1-Score  Tiempo (s)\n",
    "#Promedio    0.6023     0.7797  0.6023    0.6639      0.2865\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bf4456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  fin modelos validacion üíØ üîö-----------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
